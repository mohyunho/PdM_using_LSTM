{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'\n",
    "\n",
    "\n",
    "# read training data - It is the aircraft engine run-to-failure data. (failure and testing)\n",
    "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
    "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "train_df = pd.read_csv('Dataset/PM_train.txt', sep=\" \", header=None)\n",
    "test_df = pd.read_csv('Dataset/PM_test.txt', sep=\" \", header=None)\n",
    "truth_df = pd.read_csv('Dataset/PM_truth.txt', sep=\" \", header=None)\n",
    "\n",
    "# Drop missing data columns(redundant)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "# Sorting and indicating columns\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "\n",
    "# print (\"train_df  \\n\", train_df)\n",
    "# print (\"test_df \\n\", test_df)\n",
    "# print (\"truth_df \\n\", truth_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n",
      "0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n",
      "1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n",
      "2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n",
      "3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n",
      "4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n",
      "\n",
      "    s5  ...       s13       s14       s15  s16       s17  s18  s19       s20  \\\n",
      "0  0.0  ...  0.220588  0.132160  0.308965  0.0  0.333333  0.0  0.0  0.558140   \n",
      "1  0.0  ...  0.264706  0.204768  0.213159  0.0  0.416667  0.0  0.0  0.682171   \n",
      "2  0.0  ...  0.220588  0.155640  0.458638  0.0  0.416667  0.0  0.0  0.728682   \n",
      "3  0.0  ...  0.250000  0.170090  0.257022  0.0  0.250000  0.0  0.0  0.666667   \n",
      "4  0.0  ...  0.220588  0.152751  0.300885  0.0  0.166667  0.0  0.0  0.658915   \n",
      "\n",
      "        s21  cycle_norm  \n",
      "0  0.661834     0.00000  \n",
      "1  0.686827     0.00277  \n",
      "2  0.721348     0.00554  \n",
      "3  0.662110     0.00831  \n",
      "4  0.716377     0.01108  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# Data Preprocessing\n",
    "##################################\n",
    "\n",
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index() # fail occurred cycle eahc R2F simulation\n",
    "rul.columns = ['id', 'max'] # assign column labels\n",
    "train_df = train_df.merge(rul, on=['id'], how='left') # merge the cycle of failure for each id\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle'] # Calulate and assign RUL\n",
    "train_df.drop('max', axis=1, inplace=True) #Drop the cycle of failure for each id(max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification,\n",
    "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "# column 'RUL' is used for regression / 'label1' and 'label2' are used for classification\n",
    "w1 = 30 # binary classification  - going to fail within 30 cycles?\n",
    "w0 = 15 # binary classification  - going to fail within 15 cycles?\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0)\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "\n",
    "\n",
    "## preprocessing(normalization) with sklearn lib for using DL\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id', 'cycle', 'RUL', 'label1', 'label2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns=train_df.columns)\n",
    "\n",
    "# processed data save into csv if needed\n",
    "# train_df.to_csv('PredictiveManteinanceEngineTraining.csv', encoding='utf-8',index = None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# TEST\n",
    "######\n",
    "## preprocessing(normalization) with sklearn lib for using DL\n",
    "# MinMax normalization (from 0 to 1)\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
    "                            columns=cols_normalize,\n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns=test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data, for calculating RUL\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data (It is for classification)\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0)\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# test_df.to_csv('PredictiveManteinanceEngineValidation.csv', encoding='utf-8',index = None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "(15631, 50, 25)\n"
     ]
    }
   ],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "\n",
    "# function to reshape features into (samples, time steps, features)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n",
    "# pick the feature columns\n",
    "sensor_cols = ['s' + str(i) for i in range(1, 22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "\n",
    "# TODO for debug\n",
    "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
    "val = list(gen_sequence(train_df[train_df['id'] == 1], sequence_length, sequence_cols))\n",
    "print(len(val))\n",
    "\n",
    "# generator for the sequences\n",
    "# transform each id of the train dataset in a sequence\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id'] == id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x7f35008a39d0>\n",
      "len(label_gen) 100\n",
      "label_array.shape (15631, 1)\n",
      "df_one \n",
      "      id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
      "0     1      1  0.459770  0.166667       0.0  0.0  0.183735  0.406802   \n",
      "1     1      2  0.609195  0.250000       0.0  0.0  0.283133  0.453019   \n",
      "2     1      3  0.252874  0.750000       0.0  0.0  0.343373  0.369523   \n",
      "3     1      4  0.540230  0.500000       0.0  0.0  0.343373  0.256159   \n",
      "4     1      5  0.390805  0.333333       0.0  0.0  0.349398  0.257467   \n",
      "..   ..    ...       ...       ...       ...  ...       ...       ...   \n",
      "187   1    188  0.114943  0.750000       0.0  0.0  0.765060  0.683235   \n",
      "188   1    189  0.465517  0.666667       0.0  0.0  0.894578  0.547853   \n",
      "189   1    190  0.344828  0.583333       0.0  0.0  0.731928  0.614345   \n",
      "190   1    191  0.500000  0.166667       0.0  0.0  0.641566  0.682799   \n",
      "191   1    192  0.551724  0.500000       0.0  0.0  0.701807  0.662089   \n",
      "\n",
      "           s4   s5  ...  s16       s17  s18  s19       s20       s21  RUL  \\\n",
      "0    0.309757  0.0  ...  0.0  0.333333  0.0  0.0  0.713178  0.724662  191   \n",
      "1    0.352633  0.0  ...  0.0  0.333333  0.0  0.0  0.666667  0.731014  190   \n",
      "2    0.370527  0.0  ...  0.0  0.166667  0.0  0.0  0.627907  0.621375  189   \n",
      "3    0.331195  0.0  ...  0.0  0.333333  0.0  0.0  0.573643  0.662386  188   \n",
      "4    0.404625  0.0  ...  0.0  0.416667  0.0  0.0  0.589147  0.704502  187   \n",
      "..        ...  ...  ...  ...       ...  ...  ...       ...       ...  ...   \n",
      "187  0.684166  0.0  ...  0.0  0.666667  0.0  0.0  0.286822  0.089202    4   \n",
      "188  0.772451  0.0  ...  0.0  0.583333  0.0  0.0  0.263566  0.301712    3   \n",
      "189  0.737677  0.0  ...  0.0  0.833333  0.0  0.0  0.271318  0.239299    2   \n",
      "190  0.734639  0.0  ...  0.0  0.500000  0.0  0.0  0.240310  0.324910    1   \n",
      "191  0.758778  0.0  ...  0.0  0.666667  0.0  0.0  0.263566  0.097625    0   \n",
      "\n",
      "     label1  label2  cycle_norm  \n",
      "0         0       0    0.000000  \n",
      "1         0       0    0.002770  \n",
      "2         0       0    0.005540  \n",
      "3         0       0    0.008310  \n",
      "4         0       0    0.011080  \n",
      "..      ...     ...         ...  \n",
      "187       1       2    0.518006  \n",
      "188       1       2    0.520776  \n",
      "189       1       2    0.523546  \n",
      "190       1       2    0.526316  \n",
      "191       1       2    0.529086  \n",
      "\n",
      "[192 rows x 30 columns]\n",
      "df_one[['RUL']] [191 190 189 188 187 186 185 184 183 182 181 180 179 178 177 176 175 174\n",
      " 173 172 171 170 169 168 167 166 165 164 163 162 161 160 159 158 157 156\n",
      " 155 154 153 152 151 150 149 148 147 146 145 144 143 142 141 140 139 138\n",
      " 137 136 135 134 133 132 131 130 129 128 127 126 125 124 123 122 121 120\n",
      " 119 118 117 116 115 114 113 112 111 110 109 108 107 106 105 104 103 102\n",
      " 101 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84\n",
      "  83  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66\n",
      "  65  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48\n",
      "  47  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30\n",
      "  29  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12\n",
      "  11  10   9   8   7   6   5   4   3   2   1   0]\n",
      "df_one[['RUL']] [[191]\n",
      " [190]\n",
      " [189]\n",
      " [188]\n",
      " [187]\n",
      " [186]\n",
      " [185]\n",
      " [184]\n",
      " [183]\n",
      " [182]\n",
      " [181]\n",
      " [180]\n",
      " [179]\n",
      " [178]\n",
      " [177]\n",
      " [176]\n",
      " [175]\n",
      " [174]\n",
      " [173]\n",
      " [172]\n",
      " [171]\n",
      " [170]\n",
      " [169]\n",
      " [168]\n",
      " [167]\n",
      " [166]\n",
      " [165]\n",
      " [164]\n",
      " [163]\n",
      " [162]\n",
      " [161]\n",
      " [160]\n",
      " [159]\n",
      " [158]\n",
      " [157]\n",
      " [156]\n",
      " [155]\n",
      " [154]\n",
      " [153]\n",
      " [152]\n",
      " [151]\n",
      " [150]\n",
      " [149]\n",
      " [148]\n",
      " [147]\n",
      " [146]\n",
      " [145]\n",
      " [144]\n",
      " [143]\n",
      " [142]\n",
      " [141]\n",
      " [140]\n",
      " [139]\n",
      " [138]\n",
      " [137]\n",
      " [136]\n",
      " [135]\n",
      " [134]\n",
      " [133]\n",
      " [132]\n",
      " [131]\n",
      " [130]\n",
      " [129]\n",
      " [128]\n",
      " [127]\n",
      " [126]\n",
      " [125]\n",
      " [124]\n",
      " [123]\n",
      " [122]\n",
      " [121]\n",
      " [120]\n",
      " [119]\n",
      " [118]\n",
      " [117]\n",
      " [116]\n",
      " [115]\n",
      " [114]\n",
      " [113]\n",
      " [112]\n",
      " [111]\n",
      " [110]\n",
      " [109]\n",
      " [108]\n",
      " [107]\n",
      " [106]\n",
      " [105]\n",
      " [104]\n",
      " [103]\n",
      " [102]\n",
      " [101]\n",
      " [100]\n",
      " [ 99]\n",
      " [ 98]\n",
      " [ 97]\n",
      " [ 96]\n",
      " [ 95]\n",
      " [ 94]\n",
      " [ 93]\n",
      " [ 92]\n",
      " [ 91]\n",
      " [ 90]\n",
      " [ 89]\n",
      " [ 88]\n",
      " [ 87]\n",
      " [ 86]\n",
      " [ 85]\n",
      " [ 84]\n",
      " [ 83]\n",
      " [ 82]\n",
      " [ 81]\n",
      " [ 80]\n",
      " [ 79]\n",
      " [ 78]\n",
      " [ 77]\n",
      " [ 76]\n",
      " [ 75]\n",
      " [ 74]\n",
      " [ 73]\n",
      " [ 72]\n",
      " [ 71]\n",
      " [ 70]\n",
      " [ 69]\n",
      " [ 68]\n",
      " [ 67]\n",
      " [ 66]\n",
      " [ 65]\n",
      " [ 64]\n",
      " [ 63]\n",
      " [ 62]\n",
      " [ 61]\n",
      " [ 60]\n",
      " [ 59]\n",
      " [ 58]\n",
      " [ 57]\n",
      " [ 56]\n",
      " [ 55]\n",
      " [ 54]\n",
      " [ 53]\n",
      " [ 52]\n",
      " [ 51]\n",
      " [ 50]\n",
      " [ 49]\n",
      " [ 48]\n",
      " [ 47]\n",
      " [ 46]\n",
      " [ 45]\n",
      " [ 44]\n",
      " [ 43]\n",
      " [ 42]\n",
      " [ 41]\n",
      " [ 40]\n",
      " [ 39]\n",
      " [ 38]\n",
      " [ 37]\n",
      " [ 36]\n",
      " [ 35]\n",
      " [ 34]\n",
      " [ 33]\n",
      " [ 32]\n",
      " [ 31]\n",
      " [ 30]\n",
      " [ 29]\n",
      " [ 28]\n",
      " [ 27]\n",
      " [ 26]\n",
      " [ 25]\n",
      " [ 24]\n",
      " [ 23]\n",
      " [ 22]\n",
      " [ 21]\n",
      " [ 20]\n",
      " [ 19]\n",
      " [ 18]\n",
      " [ 17]\n",
      " [ 16]\n",
      " [ 15]\n",
      " [ 14]\n",
      " [ 13]\n",
      " [ 12]\n",
      " [ 11]\n",
      " [ 10]\n",
      " [  9]\n",
      " [  8]\n",
      " [  7]\n",
      " [  6]\n",
      " [  5]\n",
      " [  4]\n",
      " [  3]\n",
      " [  2]\n",
      " [  1]\n",
      " [  0]]\n"
     ]
    }
   ],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. \n",
    "    This means for testing we need to drop those which are below the window-length. \n",
    "    An alternative would be to pad  sequences so that we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]]\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id'] == id], sequence_length, ['RUL'])\n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "print (gen_labels(train_df[train_df['id'] == id], sequence_length, ['RUL'])\n",
    "             for id in train_df['id'].unique())\n",
    "\n",
    "# print (\"label_gen\", label_gen)\n",
    "print (\"len(label_gen)\", len(label_gen))\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "print (\"label_array.shape\", label_array.shape)\n",
    "\n",
    "\n",
    "\n",
    "df_one = train_df[train_df['id'] == 1]\n",
    "print (\"df_one \\n\",df_one )\n",
    "\n",
    "print (\"df_one[['RUL']]\", df_one['RUL'].values)\n",
    "print (\"df_one[['RUL']]\", df_one[['RUL']].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50, 100)           50400     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 51        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 80,651\n",
      "Trainable params: 80,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination\n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Next, we build a deep network.\n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.\n",
    "# Dropout is also applied after each LSTM layer to control overfitting.\n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "keras.backend.backend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
