{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'\n",
    "\n",
    "\n",
    "# read training data - It is the aircraft engine run-to-failure data. (failure and testing)\n",
    "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
    "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "train_df = pd.read_csv('Dataset/PM_train.txt', sep=\" \", header=None)\n",
    "test_df = pd.read_csv('Dataset/PM_test.txt', sep=\" \", header=None)\n",
    "truth_df = pd.read_csv('Dataset/PM_truth.txt', sep=\" \", header=None)\n",
    "\n",
    "# Drop missing data columns(redundant)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "# Sorting and indicating columns\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "\n",
    "# print (\"train_df  \\n\", train_df)\n",
    "# print (\"test_df \\n\", test_df)\n",
    "# print (\"truth_df \\n\", truth_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  cycle  setting1  setting2  setting3   s1        s2        s3        s4  \\\n",
      "0   1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661  0.269413   \n",
      "1   1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551  0.222316   \n",
      "2   1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632  0.322248   \n",
      "3   1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154  0.408001   \n",
      "4   1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082  0.332039   \n",
      "\n",
      "    s5  ...       s13       s14       s15  s16       s17  s18  s19       s20  \\\n",
      "0  0.0  ...  0.220588  0.132160  0.308965  0.0  0.333333  0.0  0.0  0.558140   \n",
      "1  0.0  ...  0.264706  0.204768  0.213159  0.0  0.416667  0.0  0.0  0.682171   \n",
      "2  0.0  ...  0.220588  0.155640  0.458638  0.0  0.416667  0.0  0.0  0.728682   \n",
      "3  0.0  ...  0.250000  0.170090  0.257022  0.0  0.250000  0.0  0.0  0.666667   \n",
      "4  0.0  ...  0.220588  0.152751  0.300885  0.0  0.166667  0.0  0.0  0.658915   \n",
      "\n",
      "        s21  cycle_norm  \n",
      "0  0.661834     0.00000  \n",
      "1  0.686827     0.00277  \n",
      "2  0.721348     0.00554  \n",
      "3  0.662110     0.00831  \n",
      "4  0.716377     0.01108  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "# Data Preprocessing\n",
    "##################################\n",
    "\n",
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index() # fail occurred cycle eahc R2F simulation\n",
    "rul.columns = ['id', 'max'] # assign column labels\n",
    "train_df = train_df.merge(rul, on=['id'], how='left') # merge the cycle of failure for each id\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle'] # Calulate and assign RUL\n",
    "train_df.drop('max', axis=1, inplace=True) #Drop the cycle of failure for each id(max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification,\n",
    "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "# column 'RUL' is used for regression / 'label1' and 'label2' are used for classification\n",
    "w1 = 30 # binary classification  - going to fail within 30 cycles?\n",
    "w0 = 15 # binary classification  - going to fail within 15 cycles?\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0)\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "\n",
    "\n",
    "## preprocessing(normalization) with sklearn lib for using DL\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id', 'cycle', 'RUL', 'label1', 'label2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns=train_df.columns)\n",
    "\n",
    "# processed data save into csv if needed\n",
    "# train_df.to_csv('PredictiveManteinanceEngineTraining.csv', encoding='utf-8',index = None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# TEST\n",
    "######\n",
    "## preprocessing(normalization) with sklearn lib for using DL\n",
    "# MinMax normalization (from 0 to 1)\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
    "                            columns=cols_normalize,\n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns=test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data, for calculating RUL\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data (It is for classification)\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0)\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# test_df.to_csv('PredictiveManteinanceEngineValidation.csv', encoding='utf-8',index = None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence_cols \n",
      " ['setting1', 'setting2', 'setting3', 'cycle_norm', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
      "seq_gen <generator object <genexpr> at 0x7ff09d8c6050>\n",
      "type(seq_gen) <class 'generator'>\n",
      "type(list(seq_gen)) <class 'list'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-10bb422235b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# generate sequences and convert to numpy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mseq_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "\n",
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "\n",
    "# function to reshape features into (samples, time steps, features)\n",
    "# The output of the function is a number of numpy array with fixed sequence length(window size) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    # only take features and its values (convert DataFrame to numpy array)\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0] # number of rows in the array (the number of instances for each machine)\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,142),(50,192)\n",
    "    # zip iteration : Iterate over multiple lists simultaneously\n",
    "    # Iterate minimum among all the lists  \n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 142 191 -> from row 142 to 191\n",
    "    # 142 number of numpy array, each has the shape of (sequence length, number of features) = (50,25)\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n",
    "# pick the feature columns (exclude RUL and labels )\n",
    "sensor_cols = ['s' + str(i) for i in range(1, 22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "\n",
    "print (\"sequence_cols \\n\",  sequence_cols)\n",
    "\n",
    "\n",
    "# id_df = train_df[train_df['id'] == 1]\n",
    "# print (\"id_df \\n\", id_df)\n",
    "\n",
    "\n",
    "# print (\"id_df[sequence_cols] \\n\", id_df[sequence_cols])\n",
    "\n",
    "\n",
    "\n",
    "# print (\"id_df[sequence_cols].values \\n\", id_df[sequence_cols].values)\n",
    "\n",
    "\n",
    "# id_df_values = id_df[sequence_cols].values\n",
    "\n",
    "# print (\"type(id_df_values)\", type(id_df_values))\n",
    "# print (\"id_df_values.shape\", id_df_values.shape)\n",
    "\n",
    "\n",
    "\n",
    "# num_elements = id_df_values.shape[0]\n",
    "# seq_length = sequence_length\n",
    "# i = 0\n",
    "# for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "#     print (\"i: \", i)\n",
    "#     print (\"start,stop:\", start, stop)\n",
    "#     print (\"id_df_values[start:stop, :] \\n\", id_df_values[start:stop, :]) \n",
    "#     i +=1\n",
    "    \n",
    "\n",
    "# print (\"range(0, num_elements - seq_length) \\n\", range(0, num_elements - seq_length))\n",
    "# print (\"range(seq_length, num_elements) \\n\", range(seq_length, num_elements))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
    "\n",
    "\n",
    "\n",
    "# val = list(gen_sequence(train_df[train_df['id'] == 1], sequence_length, sequence_cols))\n",
    "# print(len(val))\n",
    "\n",
    "# print (\"type(val)\", type(val))\n",
    "# print (\"val[0]\", val[0])\n",
    "# print (\"type(val[0])\", type(val[0]))\n",
    "# print (\"val[0].shape\", val[0].shape)\n",
    "\n",
    "# generator for the sequences\n",
    "# transform each id of the train dataset in a sequence\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id'] == id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "\n",
    "\n",
    "print (\"seq_gen\", seq_gen)\n",
    "print (\"type(seq_gen)\", type(seq_gen))\n",
    "\n",
    "print (\"type(list(seq_gen))\", type(list(seq_gen)))\n",
    "\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n",
    "\n",
    "\n",
    "\n",
    "print(\"end of lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. \n",
    "    This means for testing we need to drop those which are below the window-length. \n",
    "    An alternative would be to pad  sequences so that we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]]\n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id'] == id], sequence_length, ['RUL'])\n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "print (gen_labels(train_df[train_df['id'] == id], sequence_length, ['RUL'])\n",
    "             for id in train_df['id'].unique())\n",
    "\n",
    "# print (\"label_gen\", label_gen)\n",
    "print (\"len(label_gen)\", len(label_gen))\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "print (\"label_array.shape\", label_array.shape)\n",
    "\n",
    "\n",
    "\n",
    "df_one = train_df[train_df['id'] == 1]\n",
    "print (\"df_one \\n\",df_one )\n",
    "\n",
    "print (\"df_one[['RUL']]\", df_one['RUL'].values)\n",
    "print (\"df_one[['RUL']]\", df_one[['RUL']].values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination\n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Next, we build a deep network.\n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units.\n",
    "# Dropout is also applied after each LSTM layer to control overfitting.\n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
