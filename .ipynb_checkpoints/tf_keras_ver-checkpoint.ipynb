{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'loaRd_model' from 'keras.models' (/home/hyunhomo/anaconda3/lib/python3.7/site-packages/keras/models.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c8a44f9c19e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloaRd_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'loaRd_model' from 'keras.models' (/home/hyunhomo/anaconda3/lib/python3.7/site-packages/keras/models.py)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.backend as K\n",
    "from keras.layers.core import Activation\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)\n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'regression_model.h5'\n",
    "\n",
    "\n",
    "# read training data - It is the aircraft engine run-to-failure data. (failure and testing)\n",
    "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
    "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "train_df = pd.read_csv('Dataset/PM_train.txt', sep=\" \", header=None)\n",
    "test_df = pd.read_csv('Dataset/PM_test.txt', sep=\" \", header=None)\n",
    "truth_df = pd.read_csv('Dataset/PM_truth.txt', sep=\" \", header=None)\n",
    "\n",
    "# Drop missing data columns(redundant)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "# Sorting and indicating columns\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "\n",
    "# print (\"train_df  \\n\", train_df)\n",
    "# print (\"test_df \\n\", test_df)\n",
    "# print (\"truth_df \\n\", truth_df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "# Data Preprocessing\n",
    "##################################\n",
    "\n",
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index() # fail occurred cycle eahc R2F simulation\n",
    "rul.columns = ['id', 'max'] # assign column labels\n",
    "train_df = train_df.merge(rul, on=['id'], how='left') # merge the cycle of failure for each id\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle'] # Calulate and assign RUL\n",
    "train_df.drop('max', axis=1, inplace=True) #Drop the cycle of failure for each id(max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification,\n",
    "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "# column 'RUL' is used for regression / 'label1' and 'label2' are used for classification\n",
    "w1 = 30 # binary classification  - going to fail within 30 cycles?\n",
    "w0 = 15 # binary classification  - going to fail within 15 cycles?\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0)\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "\n",
    "\n",
    "## preprocessing(normalization) with sklearn lib for using DL\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id', 'cycle', 'RUL', 'label1', 'label2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]),\n",
    "                             columns=cols_normalize,\n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns=train_df.columns)\n",
    "\n",
    "# processed data save into csv if needed\n",
    "# train_df.to_csv('PredictiveManteinanceEngineTraining.csv', encoding='utf-8',index = None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######\n",
    "# TEST\n",
    "######\n",
    "## preprocessing(normalization) with sklearn lib for using DL\n",
    "# MinMax normalization (from 0 to 1)\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]),\n",
    "                            columns=cols_normalize,\n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns=test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "print(test_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data, for calculating RUL\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data (It is for classification)\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0)\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "# test_df.to_csv('PredictiveManteinanceEngineValidation.csv', encoding='utf-8',index = None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "\n",
    "# function to reshape features into (samples, time steps, features)\n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements - seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "\n",
    "\n",
    "# pick the feature columns\n",
    "sensor_cols = ['s' + str(i) for i in range(1, 22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "\n",
    "# TODO for debug\n",
    "# val is a list of 192 - 50 = 142 bi-dimensional array (50 rows x 25 columns)\n",
    "val = list(gen_sequence(train_df[train_df['id'] == 1], sequence_length, sequence_cols))\n",
    "print(len(val))\n",
    "\n",
    "# generator for the sequences\n",
    "# transform each id of the train dataset in a sequence\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id'] == id], sequence_length, sequence_cols))\n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "print(seq_array.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target.\n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['RUL']) \n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen).astype(np.float32)\n",
    "label_array.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_keras(y_true, y_pred):\n",
    "    \"\"\"Coefficient of Determination \n",
    "    \"\"\"\n",
    "    SS_res =  K.sum(K.square( y_true - y_pred ))\n",
    "    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )\n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )\n",
    "\n",
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and linear activation since this is a regression problem.\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(\n",
    "          units=50,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=nb_out))\n",
    "model.add(Activation(\"linear\"))\n",
    "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae',r2_keras])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# summarize history for R^2\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['r2_keras'])\n",
    "plt.plot(history.history['val_r2_keras'])\n",
    "plt.title('model r^2')\n",
    "plt.ylabel('R^2')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_r2.png\")\n",
    "\n",
    "# summarize history for MAE\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['mae'])\n",
    "plt.plot(history.history['val_mae'])\n",
    "plt.title('model MAE')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_mae.png\")\n",
    "\n",
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_regression_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('\\nMAE: {}'.format(scores[1]))\n",
    "print('\\nR^2: {}'.format(scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred = model.predict(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "\n",
    "test_set = pd.DataFrame(y_pred)\n",
    "test_set.to_csv('submit_train.csv', index = None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the last sequence for each id in the test data\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "#print(\"seq_array_test_last\")\n",
    "#print(seq_array_test_last)\n",
    "#print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "#print(\"y_mask\")\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "label_array_test_last = test_df.groupby('id')['RUL'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "#print(label_array_test_last.shape)\n",
    "#print(\"label_array_test_last\")\n",
    "#print(label_array_test_last)\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path,custom_objects={'r2_keras': r2_keras})\n",
    "\n",
    "    # test metrics\n",
    "    scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "    print('\\nMAE: {}'.format(scores_test[1]))\n",
    "    print('\\nR^2: {}'.format(scores_test[2]))\n",
    "\n",
    "    y_pred_test = estimator.predict(seq_array_test_last)\n",
    "    y_true_test = label_array_test_last\n",
    "\n",
    "    test_set = pd.DataFrame(y_pred_test)\n",
    "    test_set.to_csv('submit_test.csv', index = None)\n",
    "\n",
    "    # Plot in blue color the predicted data and in green color the\n",
    "    # actual data to verify visually the accuracy of the model.\n",
    "    fig_verify = plt.figure(figsize=(10, 5))\n",
    "    plt.plot(y_pred_test, color=\"blue\")\n",
    "    plt.plot(y_true_test, color=\"green\")\n",
    "    plt.title('prediction')\n",
    "    plt.ylabel('value')\n",
    "    plt.xlabel('row')\n",
    "    plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "    plt.show()\n",
    "    fig_verify.savefig(\"model_regression_verify.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
