{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(1234)  \n",
    "PYTHONHASHSEED = 0\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "# define path to save model\n",
    "model_path = 'binary_model.h5'\n",
    "\n",
    "# read training data - It is the aircraft engine run-to-failure data. (failure and testing)\n",
    "# read test data - It is the aircraft engine operating data without failure events recorded.\n",
    "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
    "train_df = pd.read_csv('Dataset/PM_train.txt', sep=\" \", header=None)\n",
    "test_df = pd.read_csv('Dataset/PM_test.txt', sep=\" \", header=None)\n",
    "truth_df = pd.read_csv('Dataset/PM_truth.txt', sep=\" \", header=None)\n",
    "\n",
    "# Drop missing data columns(redundant)\n",
    "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_df.drop(test_df.columns[[26, 27]], axis=1, inplace=True)\n",
    "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)\n",
    "\n",
    "# Sorting and indicating columns\n",
    "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "train_df = train_df.sort_values(['id','cycle'])\n",
    "\n",
    "test_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
    "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
    "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(20631,)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "(20631, 3)\n",
      "        id  cycle  setting1  setting2  setting3      s1      s2       s3  \\\n",
      "0        1      1   -0.0007   -0.0004     100.0  518.67  641.82  1589.70   \n",
      "1        1      2    0.0019   -0.0003     100.0  518.67  642.15  1591.82   \n",
      "2        1      3   -0.0043    0.0003     100.0  518.67  642.35  1587.99   \n",
      "3        1      4    0.0007    0.0000     100.0  518.67  642.35  1582.79   \n",
      "4        1      5   -0.0019   -0.0002     100.0  518.67  642.37  1582.85   \n",
      "...    ...    ...       ...       ...       ...     ...     ...      ...   \n",
      "20626  100    196   -0.0004   -0.0003     100.0  518.67  643.49  1597.98   \n",
      "20627  100    197   -0.0016   -0.0005     100.0  518.67  643.54  1604.50   \n",
      "20628  100    198    0.0004    0.0000     100.0  518.67  643.42  1602.46   \n",
      "20629  100    199   -0.0011    0.0003     100.0  518.67  643.23  1605.26   \n",
      "20630  100    200   -0.0032   -0.0005     100.0  518.67  643.85  1600.38   \n",
      "\n",
      "            s4     s5  ...   s18    s19    s20      s21  RUL  label1  label2  \\\n",
      "0      1400.60  14.62  ...  2388  100.0  39.06  23.4190  191       0       0   \n",
      "1      1403.14  14.62  ...  2388  100.0  39.00  23.4236  190       0       0   \n",
      "2      1404.20  14.62  ...  2388  100.0  38.95  23.3442  189       0       0   \n",
      "3      1401.87  14.62  ...  2388  100.0  38.88  23.3739  188       0       0   \n",
      "4      1406.22  14.62  ...  2388  100.0  38.90  23.4044  187       0       0   \n",
      "...        ...    ...  ...   ...    ...    ...      ...  ...     ...     ...   \n",
      "20626  1428.63  14.62  ...  2388  100.0  38.49  22.9735    4       1       2   \n",
      "20627  1433.58  14.62  ...  2388  100.0  38.30  23.1594    3       1       2   \n",
      "20628  1428.18  14.62  ...  2388  100.0  38.44  22.9333    2       1       2   \n",
      "20629  1426.53  14.62  ...  2388  100.0  38.29  23.0640    1       1       2   \n",
      "20630  1432.14  14.62  ...  2388  100.0  38.37  23.0522    0       1       2   \n",
      "\n",
      "       class0  class1  class2  \n",
      "0         1.0     0.0     0.0  \n",
      "1         1.0     0.0     0.0  \n",
      "2         1.0     0.0     0.0  \n",
      "3         1.0     0.0     0.0  \n",
      "4         1.0     0.0     0.0  \n",
      "...       ...     ...     ...  \n",
      "20626     0.0     0.0     1.0  \n",
      "20627     0.0     0.0     1.0  \n",
      "20628     0.0     0.0     1.0  \n",
      "20629     0.0     0.0     1.0  \n",
      "20630     0.0     0.0     1.0  \n",
      "\n",
      "[20631 rows x 32 columns]\n",
      "<class 'numpy.ndarray'>\n",
      "(13096,)\n",
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "(13096, 3)\n",
      "        id  cycle  setting1  setting2  setting3   s1        s2        s3  \\\n",
      "0        1      1  0.632184  0.750000       0.0  0.0  0.545181  0.310661   \n",
      "1        1      2  0.344828  0.250000       0.0  0.0  0.150602  0.379551   \n",
      "2        1      3  0.517241  0.583333       0.0  0.0  0.376506  0.346632   \n",
      "3        1      4  0.741379  0.500000       0.0  0.0  0.370482  0.285154   \n",
      "4        1      5  0.580460  0.500000       0.0  0.0  0.391566  0.352082   \n",
      "...    ...    ...       ...       ...       ...  ...       ...       ...   \n",
      "13091  100    194  0.781609  0.500000       0.0  0.0  0.611446  0.619359   \n",
      "13092  100    195  0.436782  0.416667       0.0  0.0  0.605422  0.537388   \n",
      "13093  100    196  0.465517  0.250000       0.0  0.0  0.671687  0.482014   \n",
      "13094  100    197  0.281609  0.583333       0.0  0.0  0.617470  0.522128   \n",
      "13095  100    198  0.574713  0.750000       0.0  0.0  0.524096  0.666667   \n",
      "\n",
      "             s4   s5  ...  s19       s20       s21  cycle_norm  RUL  label1  \\\n",
      "0      0.269413  0.0  ...  0.0  0.558140  0.661834    0.000000  142       0   \n",
      "1      0.222316  0.0  ...  0.0  0.682171  0.686827    0.002770  141       0   \n",
      "2      0.322248  0.0  ...  0.0  0.728682  0.721348    0.005540  140       0   \n",
      "3      0.408001  0.0  ...  0.0  0.666667  0.662110    0.008310  139       0   \n",
      "4      0.332039  0.0  ...  0.0  0.658915  0.716377    0.011080  138       0   \n",
      "...         ...  ...  ...  ...       ...       ...         ...  ...     ...   \n",
      "13091  0.566172  0.0  ...  0.0  0.395349  0.418669    0.534626   24       1   \n",
      "13092  0.671843  0.0  ...  0.0  0.333333  0.528721    0.537396   23       1   \n",
      "13093  0.414754  0.0  ...  0.0  0.372093  0.429301    0.540166   22       1   \n",
      "13094  0.626435  0.0  ...  0.0  0.403101  0.518779    0.542936   21       1   \n",
      "13095  0.721472  0.0  ...  0.0  0.434109  0.402237    0.545706   20       1   \n",
      "\n",
      "       label2  class0  class1  class2  \n",
      "0           0     1.0     0.0     0.0  \n",
      "1           0     1.0     0.0     0.0  \n",
      "2           0     1.0     0.0     0.0  \n",
      "3           0     1.0     0.0     0.0  \n",
      "4           0     1.0     0.0     0.0  \n",
      "...       ...     ...     ...     ...  \n",
      "13091       1     0.0     1.0     0.0  \n",
      "13092       1     0.0     1.0     0.0  \n",
      "13093       1     0.0     1.0     0.0  \n",
      "13094       1     0.0     1.0     0.0  \n",
      "13095       1     0.0     1.0     0.0  \n",
      "\n",
      "[13096 rows x 33 columns]\n"
     ]
    }
   ],
   "source": [
    "#######\n",
    "# TRAIN\n",
    "#######\n",
    "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
    "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "train_df = train_df.merge(rul, on=['id'], how='left')\n",
    "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
    "train_df.drop('max', axis=1, inplace=True)\n",
    "# generate label columns for training data\n",
    "# we will only make use of \"label1\" for binary classification, \n",
    "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
    "w1 = 30\n",
    "w0 = 15\n",
    "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
    "train_df['label2'] = train_df['label1']\n",
    "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2\n",
    "# pd.set_option('display.max_rows', 5000)\n",
    "# print (train_df.head(5000))\n",
    "\n",
    "\n",
    "## Generate multi classes for softmax multiclass classification\n",
    "# class0 = 1, if label2 is 0\n",
    "# class1 = 1, if label2 is 1\n",
    "# class2 = 1, if label2 is 2\n",
    "\n",
    "# extract label2 as numpy array\n",
    "# pd series to numpy array\n",
    "label2_array = train_df['label2'].to_numpy()\n",
    "print (type(label2_array))\n",
    "\n",
    "class0_array = np.zeros(len(label2_array))\n",
    "class1_array = np.zeros(len(label2_array))\n",
    "class2_array = np.zeros(len(label2_array))\n",
    "\n",
    "# assign '1' to generate one hot encoding vector for each class\n",
    "class0_array[label2_array==0] = 1\n",
    "class1_array[label2_array==1] = 1\n",
    "class2_array[label2_array==2] = 1\n",
    "\n",
    "#check\n",
    "print (class0_array.shape)\n",
    "print (class1_array)\n",
    "print (class2_array)\n",
    "train_df['class0'] = class0_array\n",
    "train_df['class1'] = class1_array\n",
    "train_df['class2'] = class2_array\n",
    "\n",
    "class_idx = np.stack((class0_array, class1_array, class2_array), axis=-1)\n",
    "print (class_idx.shape)\n",
    "print (train_df)\n",
    "\n",
    "# MinMax normalization (from 0 to 1)\n",
    "train_df['cycle_norm'] = train_df['cycle']\n",
    "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2',\n",
    "                                              'class0','class1','class2'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_df.index)\n",
    "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_df = join_df.reindex(columns = train_df.columns)\n",
    "\n",
    "######\n",
    "# TEST\n",
    "######\n",
    "# MinMax normalization (from 0 to 1)\n",
    "test_df['cycle_norm'] = test_df['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_df.index)\n",
    "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_df = test_join_df.reindex(columns = test_df.columns)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# We use the ground truth dataset to generate labels for the test data.\n",
    "# generate column max for test data\n",
    "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
    "rul.columns = ['id', 'max']\n",
    "truth_df.columns = ['more']\n",
    "truth_df['id'] = truth_df.index + 1\n",
    "truth_df['max'] = rul['max'] + truth_df['more']\n",
    "truth_df.drop('more', axis=1, inplace=True)\n",
    "\n",
    "# generate RUL for test data\n",
    "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
    "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
    "test_df.drop('max', axis=1, inplace=True)\n",
    "\n",
    "# generate label columns w0 and w1 for test data\n",
    "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
    "test_df['label2'] = test_df['label1']\n",
    "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2\n",
    "\n",
    "\n",
    "## Ground truth of multi class \n",
    "# extract label2 as numpy array\n",
    "# pd series to numpy array\n",
    "label2_array_test = test_df['label2'].to_numpy()\n",
    "print (type(label2_array_test))\n",
    "\n",
    "class0_array_test = np.zeros(len(label2_array_test))\n",
    "class1_array_test = np.zeros(len(label2_array_test))\n",
    "class2_array_test = np.zeros(len(label2_array_test))\n",
    "\n",
    "# assign '1' to generate one hot encoding vector for each class\n",
    "class0_array_test[label2_array_test==0] = 1\n",
    "class1_array_test[label2_array_test==1] = 1\n",
    "class2_array_test[label2_array_test==2] = 1\n",
    "\n",
    "#check\n",
    "print (class0_array_test.shape)\n",
    "print (class1_array_test)\n",
    "print (class2_array_test)\n",
    "test_df['class0'] = class0_array_test\n",
    "test_df['class1'] = class1_array_test\n",
    "test_df['class2'] = class2_array_test\n",
    "\n",
    "class_idx_test = np.stack((class0_array_test, class1_array_test, class2_array_test), axis=-1)\n",
    "print (class_idx_test.shape)\n",
    "print (test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "(15631, 3)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# pick a large window size of 50 cycles\n",
    "sequence_length = 50\n",
    "\n",
    "# function to reshape features into (samples, time steps, features) \n",
    "def gen_sequence(id_df, seq_length, seq_cols):\n",
    "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
    "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
    "    we can use shorter ones \"\"\"\n",
    "    # for one id I put all the rows in a single matrix\n",
    "    data_matrix = id_df[seq_cols].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # Iterate over two lists in parallel.\n",
    "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
    "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
    "    # 0 50 -> from row 0 to row 50\n",
    "    # 1 51 -> from row 1 to row 51\n",
    "    # 2 52 -> from row 2 to row 52\n",
    "    # ...\n",
    "    # 111 191 -> from row 111 to 191\n",
    "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
    "        yield data_matrix[start:stop, :]\n",
    "        \n",
    "# pick the feature columns \n",
    "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
    "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
    "sequence_cols.extend(sensor_cols)\n",
    "\n",
    "# generator for the sequences\n",
    "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
    "           for id in train_df['id'].unique())\n",
    "\n",
    "# generate sequences and convert to numpy array\n",
    "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
    "seq_array.shape\n",
    "\n",
    "# function to generate labels\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    # For one id I put all the labels in a single matrix.\n",
    "    # For example:\n",
    "    # [[1]\n",
    "    # [4]\n",
    "    # [1]\n",
    "    # [5]\n",
    "    # [9]\n",
    "    # ...\n",
    "    # [200]] \n",
    "    data_matrix = id_df[label].values\n",
    "    num_elements = data_matrix.shape[0]\n",
    "    # I have to remove the first seq_length labels\n",
    "    # because for one id the first sequence of seq_length size have as target\n",
    "    # the last label (the previus ones are discarded).\n",
    "    # All the next id's sequences will have associated step by step one label as target. \n",
    "    return data_matrix[seq_length:num_elements, :]\n",
    "\n",
    "# generate labels\n",
    "label_gen_multi = [gen_labels(train_df[train_df['id']==id], sequence_length, ['class0', 'class1', 'class2']) \n",
    "             for id in train_df['id'].unique()]\n",
    "\n",
    "label_array = np.concatenate(label_gen_multi).astype(np.float32)\n",
    "\n",
    "nb_features = seq_array.shape[2]\n",
    "nb_out = label_array.shape[1]\n",
    "\n",
    "print (label_array)\n",
    "print (label_array.shape)\n",
    "print (nb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 50, 200)           180800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50, 100)           120400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 60)                38640     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 183       \n",
      "=================================================================\n",
      "Total params: 340,023\n",
      "Trainable params: 340,023\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n",
      "[    0     0     1 ... 15629 15630 15630]\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "Train on 14849 samples, validate on 782 samples\n",
      "Epoch 1/100\n",
      "10400/14849 [====================>.........] - ETA: 27s - loss: 0.0000e+00 - acc: 0.1169"
     ]
    }
   ],
   "source": [
    "\n",
    "# Next, we build a deep network. \n",
    "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
    "# Dropout is also applied after each LSTM layer to control overfitting. \n",
    "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
    "# build the network\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(LSTM(\n",
    "#          input_shape=(sequence_length, nb_features),\n",
    "#          units=200,\n",
    "#          return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(LSTM(\n",
    "#          units=100,\n",
    "#          return_sequences=True))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(LSTM(\n",
    "#           units=60,\n",
    "#           return_sequences=False))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(units=nb_out, activation='sigmoid'))\n",
    "\n",
    "model.add(LSTM(\n",
    "         input_shape=(sequence_length, nb_features),\n",
    "         units=200,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(\n",
    "         units=100,\n",
    "         return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(LSTM(\n",
    "          units=60,\n",
    "          return_sequences=False))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(units=nb_out, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "# adm = optimizers.Adam(learning_rate=0.0001)\n",
    "# keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=adm, metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "print (label_array)\n",
    "print (label_array.shape)\n",
    "\n",
    "# fit the network\n",
    "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=1,\n",
    "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
    "                       keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0)]\n",
    "          )\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for Accuracy\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_accuracy.png\")\n",
    "\n",
    "# summarize history for Loss\n",
    "fig_acc = plt.figure(figsize=(10, 10))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "fig_acc.savefig(\"model_loss.png\")\n",
    "\n",
    "# training metrics\n",
    "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
    "print('Accurracy: {}'.format(scores[1]))\n",
    "\n",
    "# make predictions and compute confusion matrix\n",
    "y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
    "y_true = label_array\n",
    "\n",
    "test_set = pd.DataFrame(y_pred)\n",
    "test_set.to_csv('binary_submit_train.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "# compute precision and recall\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print( 'precision = ', precision, '\\n', 'recall = ', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the last sequence for each id in the test data\n",
    "\n",
    "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
    "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
    "\n",
    "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
    "#print(\"seq_array_test_last\")\n",
    "#print(seq_array_test_last)\n",
    "#print(seq_array_test_last.shape)\n",
    "\n",
    "# Similarly, we pick the labels\n",
    "\n",
    "#print(\"y_mask\")\n",
    "# serve per prendere solo le label delle sequenze che sono almeno lunghe 50\n",
    "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
    "#print(\"y_mask\")\n",
    "#print(y_mask)\n",
    "label_array_test_last = test_df.groupby('id')['label2'].nth(-1)[y_mask].values\n",
    "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
    "#print(label_array_test_last.shape)\n",
    "#print(\"label_array_test_last\")\n",
    "#print(label_array_test_last)\n",
    "\n",
    "# if best iteration's model was saved then load and use it\n",
    "if os.path.isfile(model_path):\n",
    "    estimator = load_model(model_path)\n",
    "\n",
    "# test metrics\n",
    "scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
    "print('Accurracy: {}'.format(scores_test[1]))\n",
    "\n",
    "# make predictions and compute confusion matrix\n",
    "y_pred_test = estimator.predict_classes(seq_array_test_last)\n",
    "y_true_test = label_array_test_last\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_set = pd.DataFrame(y_pred_test)\n",
    "test_set.to_csv('binary_submit_test.csv', index = None)\n",
    "\n",
    "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "print(cm)\n",
    "\n",
    "# compute precision and recall\n",
    "precision_test = precision_score(y_true_test, y_pred_test)\n",
    "recall_test = recall_score(y_true_test, y_pred_test)\n",
    "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "print( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )\n",
    "\n",
    "# Plot in blue color the predicted data and in green color the\n",
    "# actual data to verify visually the accuracy of the model.\n",
    "fig_verify = plt.figure(figsize=(10, 5))\n",
    "plt.plot(y_pred_test, color=\"blue\")\n",
    "plt.plot(y_true_test, color=\"green\")\n",
    "plt.title('prediction')\n",
    "plt.ylabel('value')\n",
    "plt.xlabel('row')\n",
    "plt.legend(['predicted', 'actual data'], loc='upper left')\n",
    "plt.show()\n",
    "fig_verify.savefig(\"model_verify.png\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (type(y_pred_test))\n",
    "print (type(y_true_test))\n",
    "\n",
    "print (y_pred_test.shape)\n",
    "print (y_true_test.shape)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "test_print = pd.DataFrame()\n",
    "test_print['y_pred']  = y_pred_test.flatten()\n",
    "test_print['y_truth'] = y_true_test.flatten()\n",
    "print (test_print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
